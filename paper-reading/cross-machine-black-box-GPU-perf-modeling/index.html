<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>论文阅读 | 平衡精确度和预测范围的黑盒 GPU 性能建模 - libreliu&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="简介本篇文章提出了一种跨机器，黑盒，基于微测试 (microbenchmark) 的方法来解析的对不同实现变体的 OpenCL kernel 的执行时间进行预测和最优 kernel 选择。 简单来说，本文大的思路是，收集一些 kernel 中出现的特征和对应特征在运行时会出现的频率，利用 microbenchmark 在目标平台上测量这些特征每次出现会花费的运行时间，再用一个（多重）线性模型来拟合">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读 | 平衡精确度和预测范围的黑盒 GPU 性能建模">
<meta property="og:url" content="https://blog.libreliu.info/paper-reading/cross-machine-black-box-GPU-perf-modeling/">
<meta property="og:site_name" content="libreliu&#39;s blog">
<meta property="og:description" content="简介本篇文章提出了一种跨机器，黑盒，基于微测试 (microbenchmark) 的方法来解析的对不同实现变体的 OpenCL kernel 的执行时间进行预测和最优 kernel 选择。 简单来说，本文大的思路是，收集一些 kernel 中出现的特征和对应特征在运行时会出现的频率，利用 microbenchmark 在目标平台上测量这些特征每次出现会花费的运行时间，再用一个（多重）线性模型来拟合">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-02-15T16:00:00.000Z">
<meta property="article:modified_time" content="2023-02-15T16:00:00.000Z">
<meta property="article:author" content="Libre Liu">
<meta name="twitter:card" content="summary">
  
  
    <link rel="icon" href="/css/images/logo.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  
<link rel="stylesheet" href="/css/style.css">

  

<meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="libreliu's blog" type="application/atom+xml">
</head>

<body>
  <div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/paper-summary">Paper Reading</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://blog.libreliu.info"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="paper-reading-paper-reading/cross-machine-black-box-GPU-perf-modeling" class="article article-type-paper-reading" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      论文阅读 | 平衡精确度和预测范围的黑盒 GPU 性能建模
      <small class=article-detail-date-index>&nbsp; 2023-02-16</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/paper-reading/cross-machine-black-box-GPU-perf-modeling/" class="article-date">
  <time datetime="2023-02-15T16:00:00.000Z" itemprop="datePublished">2023-02-16</time>
</a>-->
      <!-- 
--><!-- by blair 160724 -->
      <!-- by blair
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
          <p>论文标题：A mechanism for balancing accuracy and scope in cross-machine black-box GPU performance modeling</p>
          <p>论文来源：The International Journal of High Performance Computing Applications (April 2019)</p>
          <p>论文作者：James D. Stevens, Andreas Klockner</p>
          <p><a target="_blank" rel="noopener" href='https://arxiv.org/pdf/1904.09538.pdf'>论文链接</a></p>
        </blockquote>
      
      
      
        <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇文章提出了一种跨机器，黑盒，基于微测试 (microbenchmark) 的方法来解析的对不同实现变体的 OpenCL kernel 的执行时间进行预测和最优 kernel 选择。</p>
<p>简单来说，本文大的思路是，收集一些 kernel 中出现的<strong>特征</strong>和对应特征在运行时会出现的<strong>频率</strong>，利用 microbenchmark 在目标平台上测量这些<strong>特征</strong>每次出现会花费的运行时间，再用一个（多重）线性模型来拟合最后的运行时间。</p>
<p>由于文章比较长，此处将文章的大概结构列举如下：</p>
<ul>
<li>Section 1: 简介</li>
<li>Section 2: 解释性的例子</li>
<li>Section 3: 本文贡献概况</li>
<li>Section 4: 本文采用的假设和局限性</li>
<li>Section 5: <strong>收集 kernel 统计信息</strong></li>
<li>Section 6: <strong>建模 kernel 执行时间</strong></li>
<li>Section 7: <strong>校准模型参数</strong></li>
<li>Section 8: 结果展示</li>
<li>Section 9: 作者调研到的、其它相关的性能建模方法</li>
</ul>
<h2 id="本文的假设和局限性"><a href="#本文的假设和局限性" class="headerlink" title="本文的假设和局限性"></a>本文的假设和局限性</h2><p>本文提到的一些 assumptions：</p>
<ul>
<li>(usefulness) 可以帮助用户理解给定机器的性能特性，并且给优化器提供变体性能数据预测参考，同时降低需要在目标系统实际测量的数据数量</li>
<li>(accuracy) 根据检索到的相关文献显示，在本文提及的 GPU kernel 性能预测问题上，没有方法可以一致的获得小于个位数的预测误差，所以本文也设定这样的目标</li>
<li>(cost-explanatory): 和其它基于排名的方法不同 (Chen et al. (2018))，虽然本文优化的目标是在各种变体中进行选择，但是本文中模型的主要输出为运行时间，且采用比较可解释的线性模型进行建模</li>
</ul>
<p>本文提到的一些局限：</p>
<ul>
<li>硬件资源的利用率：<ul>
<li>硬件资源的利用率会影响最终的性能。比如，峰值浮点性能受 SIMD lane 使用率影响，片上状态存储器 (VGPR, Scratchpad Memory) 会影响调度槽位的利用率，进而影响延迟隐藏的能力</li>
<li>不过，采用本文的方法，基本的性能损失系数是比较容易解释和估计的。比如，实际的内存带宽利用率，以及峰值 FLOP&#x2F;s</li>
<li>即使无法达到硬件资源的全部利用，对于硬件资源利用率随参数变化相对稳定的场合，本文的模型仍然可以适用。不过对于变化的情况，让本文提出的模型适用的唯一可行方法，就是将模型的粒度调低到类似 SIMD lane 的水平，这样利用率的变化就不再相关了。ECM 系列模型就是这样考虑这个问题的。<blockquote>
<p>?</p>
</blockquote>
</li>
<li>为了简化的处理这个问题，本文采用 workgroup size 恒定为 256 的参数设定。</li>
</ul>
</li>
<li>程序建模上的简化：<ul>
<li>本文的模型中，主要检测的是基于某种特殊类别的操作 (e.g. 浮点操作，特殊类型的访存) 和检测到该特征出现的次数，其中次数被建模为 non-data-dependent 的一个特征。<ul>
<li>Polyhedrally-given loop domain?</li>
</ul>
</li>
<li>所有分支指令都假设两个分支均会执行，即假设 GPU 采用 masking 的方式进行执行。<blockquote>
<p>文章认为这和 GPU 的行为是匹配的，不过显然不完全是。较新的 GPU 是同时支持 branching 和 masking 的。masking 存在的意义是对于短分支来说，可以不打断流水线。</p>
</blockquote>
</li>
</ul>
</li>
<li>内存访问开销评估：<ul>
<li>内存访问的开销受到程序访问的局部性，以及对于 banked memory 来说的 bank 竞争问题的影响。</li>
<li>本文将内存访问切分成了两种：<ul>
<li>对于各个程序都常见的，比较简单的访存模式，用 Section 6.1.1 的办法按 interlane stride, utilization radio 和 data width 进行分类<blockquote>
<p>quasi-affine? </p>
</blockquote>
</li>
<li>对于更复杂的访存模式，在 Section 7.1.1 中提供一种单独抽出来在循环里面按该模式进行访存，并且进行测量的机制</li>
</ul>
</li>
</ul>
</li>
<li>平台无关：<ul>
<li>本文提出的系统作用于 OpenCL 上，但是相似的系统在 CUDA 上也可以比较轻松的实现。</li>
</ul>
</li>
</ul>
<h2 id="收集-kernel-统计信息"><a href="#收集-kernel-统计信息" class="headerlink" title="收集 kernel 统计信息"></a>收集 kernel 统计信息</h2><h3 id="计算每个特征的预期出现次数"><a href="#计算每个特征的预期出现次数" class="headerlink" title="计算每个特征的预期出现次数"></a>计算每个特征的预期出现次数</h3><p>前面提到，本文假设程序中出现的所有循环，其循环次数和本次运行所使用的数据无关，即 non-data-dependent。</p>
<p>这种情况下，如果要求解循环体中每个语句的运行次数，简单的做法是将所有循环展开，不过这样效率会比较低。事实上，此处可以把问题看作：在 $ d $ 维的整数空间 $ \mathrm{Z}^d $ 中，可行区域是由一些约束条件构成的超平面截出来的一个子区域，某个语句的循环次数就是在该子区域中整数格点的数目。</p>
<p>文章汇总提到，用 <code>barvinok</code> 和 <code>isl</code> 库一起，可以解决前面这个数循环体内语句执行次数的问题，其中 <code>barvinok</code> 是基于 Barvinok 算法的，这是一个比较高效的、计算有理凸多胞形中的格点数目的算法。</p>
<p>当然，还要分析好一条语句内真正进行计算或数据搬运的相应特征和次数。</p>
<blockquote>
<p>为什么要抽象成有理凸多胞形？ 这是因为真正循环的次数和 Kernel 本身的一些参数，以及 Kernel 的 Launch parameters 也有关系，这里希望带着这些参数做符号计算，让模型更有用一些（比如说，优化这些参数会变得容易）</p>
</blockquote>
<h3 id="计数粒度-count-granularity"><a href="#计数粒度-count-granularity" class="headerlink" title="计数粒度 (count granularity)"></a>计数粒度 (count granularity)</h3><p>计数粒度设计的思路是，计数出来的次数尽可能贴近真实 GPU 硬件中所执行操作的次数。</p>
<p>比如，我们知道，在 OpenCL 的调度模型中，每个 <code>sub-group</code> 会尽可能匹配 GPU 调度的最小单位，并且视硬件能力 <code>sub-group</code> 内部会支持一些 reduce 和 scatter 等原语，并且算数指令一般也是以 <code>sub-group</code> 为粒度进行调度和实现的。这样，算术指令就应该以 <code>sub-group</code> 为粒度计数。</p>
<p>当然，具体 <code>sub-group</code> 的数目是依赖具体的 Kernel launch parameters 的，不过这里对前面参数的依赖是多项式形式的 (比如 <code>work-group count / 32</code>），所以可以作为一个含参的量，让前面的循环次数计算也成为一个含参的值。</p>
<!-- TOOD: check this paragraph -->

<p>粒度有如下三种：</p>
<ul>
<li>per work-item<ul>
<li>同步障操作 (barrier synchronization)</li>
</ul>
</li>
<li>per sub-group （subgroup size 需要用户提供）<ul>
<li>片上操作：算数指令和 local memory 访问</li>
<li>uniform 访问：global memory 访问，但是 <code>lid(0)</code> stride 0，即多个线程访问同一块内存区域</li>
</ul>
</li>
<li>per work-group （没有给出例子）</li>
</ul>
<blockquote>
<p>这里的讨论很不详细，需要和下面一起看</p>
</blockquote>
<h2 id="建模-kernel-执行时间"><a href="#建模-kernel-执行时间" class="headerlink" title="建模 kernel 执行时间"></a>建模 kernel 执行时间</h2>$$
T_\text{wall}({\bf n}) = \text{feat}^\text{out}({\bf n}) \approx g(\text{feat}^\text{in}_0({\bf n}), ..., \text{feat}^\text{in}_j({\bf n}), p_0, ..., p_k)
$$

<p>其中：</p>
<ul>
<li>$ {\bf n} $ 是整个计算过程中为常数的、仅与各种变体相关的整数向量</li>
<li>$ \text{feat}^\text{in}_j({\bf n}) $ 是某种单元特征的出现次数（比如单精度 FP32 乘法数）</li>
<li>$ p_i $ 是硬件相关的校正参数</li>
<li>$ g $ 是用户提供的可微函数</li>
</ul>
<h3 id="kernel-特征"><a href="#kernel-特征" class="headerlink" title="kernel 特征"></a>kernel 特征</h3><h4 id="数据移动特征"><a href="#数据移动特征" class="headerlink" title="数据移动特征"></a>数据移动特征</h4><p>对于大多数计算 kernel 来说，数据搬运所占的开销是大头。</p>
<p>内存访问模式：</p>
<ul>
<li>内存类别：global &#x2F; local</li>
<li>访问类型：load &#x2F; store</li>
<li>the local and global strides along each thread axis in the array index<ul>
<li>也就是说，每次 <code>gid(0)</code>, <code>gid(1)</code>, <code>lid(0)</code>, <code>lid(1)</code> 自增一的时候，对 array 数组访问的偏移要分别增加多少</li>
</ul>
</li>
<li>the ratio of the number of element accesses to the number of elements accessed (access-to-footprint ratio, or AFR)<ul>
<li><code>AFR = 1</code>: every element in the footprint is accessed one time</li>
<li><code>AFR &gt; 1</code>: some elements are accessed more than once<ul>
<li>这样 Cache 就可能会对速度有加成了</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>文章中提到，解析形式的模型需要建模很多机器细节，比如 workgroup 调度，内存系统架构等，来达到和黑盒模型相似的精度。一个例子是</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> k_out = <span class="number">0</span>; k_out &lt;= ((<span class="number">-16</span> + n) / <span class="number">16</span>); ++k_out)</span><br><span class="line">  ...</span><br><span class="line">  a_fetch[...] = a[n*(<span class="number">16</span>*gid(<span class="number">1</span>) + lid(<span class="number">1</span>)) + <span class="number">16</span>*k_out + lid(<span class="number">0</span>)];</span><br><span class="line">  b_fetch[...] = b[n*(<span class="number">16</span>*k_out + lid(<span class="number">1</span>)) + <span class="number">16</span>*gid(<span class="number">0</span>) + lid(<span class="number">0</span>)];</span><br></pre></td></tr></table></figure>

<p>这个例子里面的内存访问模式如下：</p>
<table>
<thead>
<tr>
<th>Array</th>
<th>Ratio</th>
<th>Local strides</th>
<th>Global strides</th>
<th>Loop stride</th>
</tr>
</thead>
<tbody><tr>
<td>a</td>
<td>n&#x2F;16</td>
<td>{0:1, 1:n}</td>
<td>{0:0, 1:n*16}</td>
<td>16</td>
</tr>
<tr>
<td>b</td>
<td>n&#x2F;16</td>
<td>{0:1, 1:n}</td>
<td>{0:16, 1:0}</td>
<td>16*n</td>
</tr>
</tbody></table>
<p>这两个例子的性能差距在 5 倍左右。</p>
</blockquote>
<p>With this approach, a universal model for all kernels on all hardware based on kernel-level features like ours  could need a prohibitively large number of global memory access features and corresponding measurement kernels. This motivates our decision to allow proxies of “in-situ” memory accesses to be included as features, which in turn motivates our ‘work removal’ code transformation, discussed in Section 7.1.1. This transformation facilitates generation of microbenchmarks exercising memory accesses which match the access patterns found in specific computations by stripping away unrelated portions of the computation in an automated fashion.</p>
<p>Specifying Data Motion Features in the Model: 弄个 aLD, bLD, f_mem_access_tag</p>
<p>也可以手动指定，不用运行时测量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = Model(</span><br><span class="line">  <span class="string">&quot;f_cl_wall_time_nvidia_geforce&quot;</span>,</span><br><span class="line">  <span class="string">&quot;p_f32madd * f_op_float32_madd + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_f32l * f_mem_access_local_float32 + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_f32ga * f_mem_access_global_float32_load_lstrides:&#123;0:1;1:&gt;15&#125;_gstrides:&#123;0:0&#125;_afr:&gt;1 + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_f32gb * f_mem_access_global_float32_load_lstrides:&#123;0:1;1:&gt;15&#125;_gstrides:&#123;0:16&#125;_afr:&gt;1 + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_f32gc * f_mem_access_global_float32_store&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>显式语法格式如下：<code>&quot;f_mem_access_tag:&lt;mem access tag&gt;_&lt;mem type&gt;_&lt;data type&gt;_&lt;direction&gt;_lstrides:&#123;&lt;local stride constraints&gt;&#125;_gstrides:&#123;&lt;global stride constraints&gt;&#125;_afr:&lt;AFR constraint&gt;&quot;</code></p>
<h4 id="算术操作特征"><a href="#算术操作特征" class="headerlink" title="算术操作特征"></a>算术操作特征</h4><p>特征：</p>
<ul>
<li>操作类型：加法、乘法、指数</li>
<li>数据类型：float32, float64</li>
</ul>
<p>本文中的工作不考虑整数算术特征，因为在模型考虑的 kernel 变体中，整数算术只用在了数组下标计算中。</p>
<h4 id="同步特征"><a href="#同步特征" class="headerlink" title="同步特征"></a>同步特征</h4><p>特征：</p>
<ul>
<li>局部同步障 (local barriers)</li>
<li>kernel 启动</li>
</ul>
<p>这里 Local barriers 是 per work-item 的，然后根据实际程序同步的需要，可能需要进行乘以同时进行同步的 work item 数量。</p>
<p>简单来说就是，认为参与同步的 thread 越多越耗时。</p>
<blockquote>
<p>Recall that the statistics gathering module counts the number of synchronizations encountered by a single work-item, so depending on how a user intends to model execution, they may need to multiply a synchronization feature like local barriers by, e.g., the number of work-groups, a feature discussed in the next section.</p>
<p>A user might incorporate synchronization features into this model as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Model(<span class="string">&quot;f_cl_wall_time_nvidia_geforce&quot;</span>,</span><br><span class="line">  <span class="string">&quot;p_f32madd * f_op_float32_madd + &quot;</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="string">&quot;p_barrier * f_sync_barrier_local * f_thread_groups + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_launch * f_sync_kernel_launch&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

</blockquote>
<h3 id="其他特征"><a href="#其他特征" class="headerlink" title="其他特征"></a>其他特征</h3><ul>
<li>Thread groups feature<ul>
<li>给定 workgroup count，进行不同 workgroup count 间启动时间补偿</li>
</ul>
</li>
<li>OpenCL wall time feature<ul>
<li>给定 platform 和 device 下，执行 60 遍获得平均 walltime，作为输出特征</li>
<li>“We measure kernel execution time excluding any host-device transfer of data.”</li>
</ul>
</li>
</ul>
<p>一个完整的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = Model(<span class="string">&quot;f_cl_wall_time_nvidia_geforce&quot;</span>,</span><br><span class="line">  <span class="string">&quot;p_f32madd * f_op_float32_madd + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_f32l * f_mem_access_local_float32 + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_f32ga * f_mem_access_global_float32_load_lstrides :&#123;0:1;1:&gt;15&#125;_gstrides:&#123;0:0&#125;_afr:&gt;1 + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_f32gb * f_mem_access_global_float32_load_lstrides :&#123;0:1;1:&gt;15&#125;_gstrides:&#123;0:16&#125;_afr:&gt;1 + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_f32gc * f_mem_access_global_float32_store + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_barrier * f_sync_barrier_local * f_thread_groups + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_group * f_thread_groups + &quot;</span></span><br><span class="line">  <span class="string">&quot;p_launch * f_sync_kernel_launch&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="校准模型参数"><a href="#校准模型参数" class="headerlink" title="校准模型参数"></a>校准模型参数</h2><p>Work Removal Transformation: a code transformation that can extract a set of desired operations from a given computation, while maintaining overall loop structure and sufficient data flow to avoid elimination of further parts of the computation by optimizing compilers</p>
<p>Work Removal 变换会把 on-chip 工作从 kernel 中去掉，达成两方面目的：</p>
<ol>
<li>测试 on-chip work 和 global memory access 各自占用时间，决定是否要进行 latency hiding</li>
<li>测试某种特殊访存模型的时间占用</li>
</ol>
<h3 id="Measurement-kernel-设计"><a href="#Measurement-kernel-设计" class="headerlink" title="Measurement kernel 设计"></a>Measurement kernel 设计</h3><ul>
<li>Global memory access<ul>
<li>AFR &#x3D; 1: Fully specified by local strides, global strides, data size<ul>
<li>That is, patterns that do not produce a write race and not nested inside sequential loops</li>
<li>Performs global load from each of <em>a variable number of input arrays</em> using the specified access pattern</li>
<li>Each work-item then stores the sum of the input array values it fetched in a single result array</li>
<li>Params: data type, global memory array size, work-group dimensions, number of input arrays, thread index strides</li>
</ul>
</li>
<li>AFR &gt; 1:<ul>
<li>Use <strong>Work Removal Tranformation</strong> to generate dedicated measurement kernel.</li>
</ul>
</li>
</ul>
</li>
<li>Arithmetic operations<ul>
<li>First, have each work-item initialize 32 private variables of the specified data type</li>
<li>Then, perform a loop in which each iteration updates each variable using the target arithmetic operation on values from other variables<ul>
<li>This is to create structural dependency</li>
</ul>
</li>
<li>We <strong>unroll the loop by a factor of 64</strong> and <strong>arrange the variable assignment order</strong> to achieve high throughput using the approach found in the Scalable HeterOgeneous Computing (SHOC) OpenCL MaxFlops.cpp benchmark (Danalis et al. 2010).<ul>
<li>the 32 variable updates are ordered so that <strong>no assignment depends on the most recent four statements</strong><ul>
<li>32 is used because it permits maximum SIMD lane utilization &amp; prevent from spilling too many registers</li>
</ul>
</li>
<li>we <strong>sum</strong> the 32 variable values and <strong>store the result in a global array</strong> according to a <strong>user-specified memory access pattern</strong><ul>
<li>(NOTE: The actual cost can be deduced by change the runcount of arithmetic ops)</li>
<li>include the global store to avoid being optimized away</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Local memory access<ul>
<li>Tags: data type, global memory array size, iteration count, and workgroup dimensions<ul>
<li>Data type determines the local data stride</li>
</ul>
</li>
</ul>
<ol>
<li>each workitem <strong>initializes one element of a local array</strong> to the data type specified</li>
<li>Then we have it perform a loop, at each iteration moving a different element from one location in the array to another. <ul>
<li>We avoid write-races and simultaneous reads from a single memory location, and use an lid(0) stride of 1, avoiding bank conflicts.</li>
</ul>
</li>
<li>After the loop completes, <strong>each work-item writes one value from the shared array to global memory</strong></li>
</ol>
</li>
<li>Other features<ul>
<li>executes a variable number of local barriers, to measure operation overlapping behaviour (<strong>Section 7.4</strong>)</li>
<li>Empty kernel launch, to measure kernel launching overhead</li>
</ul>
</li>
</ul>
<p>文章提出，<em>Using a sufficiently high-fidelity model, we expect that users will be able to differentiate between latency-based costs of a single kernel launch and throughput-related costs that would be incurred in pipelined launches.</em></p>
<blockquote>
<p>怎么做？</p>
</blockquote>
<h3 id="计算模型参数"><a href="#计算模型参数" class="headerlink" title="计算模型参数"></a>计算模型参数</h3><p>采用最小二乘法来进行拟合，得到 feature 向量中给定 feature 的出现次数和总的运行时间的关系。</p>
<h3 id="Operation-Overlap-建模"><a href="#Operation-Overlap-建模" class="headerlink" title="Operation Overlap 建模"></a>Operation Overlap 建模</h3><p>Global memory 和 On-chip 的延迟之间是有可能互相隐藏的。</p>
<p>本文的建模基于简单的想法，即 $ \max (c_{onchip}, c_{gmem}) $，两类操作的时间求 $ \max $ 操作。</p>
<p>不过 $ \max $ 不是很可导，所以采用一个可微的近似函数来做，详情可以看论文。</p>

      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<!-- <div id="bottom-donation-section"> -->
<!-- <span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span> -->
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
<!-- </div> -->
<!-- <div class="well"> -->
  <!--
  原创文章，转载请注明： 转载自<a target="_blank" rel="noopener" href="http://52binge.github.io"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  <!-- 本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。 -->
<!-- </div> -->
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E7%9A%84%E5%81%87%E8%AE%BE%E5%92%8C%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-text">本文的假设和局限性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B6%E9%9B%86-kernel-%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF"><span class="toc-text">收集 kernel 统计信息</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%AF%8F%E4%B8%AA%E7%89%B9%E5%BE%81%E7%9A%84%E9%A2%84%E6%9C%9F%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0"><span class="toc-text">计算每个特征的预期出现次数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E6%95%B0%E7%B2%92%E5%BA%A6-count-granularity"><span class="toc-text">计数粒度 (count granularity)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E6%A8%A1-kernel-%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4"><span class="toc-text">建模 kernel 执行时间</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#kernel-%E7%89%B9%E5%BE%81"><span class="toc-text">kernel 特征</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%A7%BB%E5%8A%A8%E7%89%B9%E5%BE%81"><span class="toc-text">数据移动特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%9C%AF%E6%93%8D%E4%BD%9C%E7%89%B9%E5%BE%81"><span class="toc-text">算术操作特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E7%89%B9%E5%BE%81"><span class="toc-text">同步特征</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%89%B9%E5%BE%81"><span class="toc-text">其他特征</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%A1%E5%87%86%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">校准模型参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Measurement-kernel-%E8%AE%BE%E8%AE%A1"><span class="toc-text">Measurement kernel 设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">计算模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Operation-Overlap-%E5%BB%BA%E6%A8%A1"><span class="toc-text">Operation Overlap 建模</span></a></li></ol></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        

      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/paper-reading/auto-shader-mesh-lod/" id="article-nav-newer" class="article-nav-link-wrap">
      <div class="article-nav-title"><span>&lt;</span>&nbsp;
        
          论文阅读 | Automatic Mesh and Shader Level of Detail
        
      </div>
    </a>
  
  
    <a href="/paper-reading/learning-from-program-traces/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">论文阅读 | Learning from Shader Program Traces&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2024 Libre Liu&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a target="_blank" rel="noopener" href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script>
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true,
    },
    options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
  };
</script>
<script type="text/javascript" id="MathJax-script" src="/js/mathjax/tex-chtml.js">
</script>

    

<script src="/js/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
</body>
</html>
