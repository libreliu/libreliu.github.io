<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>论文阅读 | ICARUS: NeRF 硬件加速器 - libreliu&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="简介本篇文章介绍了 NeRF 硬件加速的实现。 NeRF 回顾  Neural Radiance Field，简称 NeRF，最开始在 ECCV 2020 上被提出，提出了以神经网络编码辐射场的一种技术，并且将其运用到了基于图片的场景重建等多个领域中，是近年来受关注度相当高的一篇工作。 NeRF 的网络部分输入为 5D: 位置 $ (x,y,z) $ 和朝向 $ (\theta, \phi) $，">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读 | ICARUS: NeRF 硬件加速器">
<meta property="og:url" content="https://blog.libreliu.info/paper-reading/icarus-nerf/">
<meta property="og:site_name" content="libreliu&#39;s blog">
<meta property="og:description" content="简介本篇文章介绍了 NeRF 硬件加速的实现。 NeRF 回顾  Neural Radiance Field，简称 NeRF，最开始在 ECCV 2020 上被提出，提出了以神经网络编码辐射场的一种技术，并且将其运用到了基于图片的场景重建等多个领域中，是近年来受关注度相当高的一篇工作。 NeRF 的网络部分输入为 5D: 位置 $ (x,y,z) $ 和朝向 $ (\theta, \phi) $，">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/nerf-overview.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/icarus-overview.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/peu-overview.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/mlp-compution-overview.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/working_principle_rmcm.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/sonb-overview.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/VRU-Overview.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/VRU-classic.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/VRU-rewrite.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/FPGA-prototyping.png">
<meta property="og:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/icarus-comparation.png">
<meta property="article:published_time" content="2022-10-06T16:00:00.000Z">
<meta property="article:modified_time" content="2022-10-16T05:59:04.150Z">
<meta property="article:author" content="Libre Liu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.libreliu.info/paper-reading/icarus-nerf/nerf-overview.png">
  
  
    <link rel="icon" href="/css/images/logo.png">
  
  <link href="/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
  <link href="/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">
  
<link rel="stylesheet" href="/css/style.css">

  

<meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="libreliu's blog" type="application/atom+xml">
</head>

<body>
  <div id="container">
    <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
      <a id="logo" class="logo" href="/"></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/paper-summary">Paper Reading</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div id="search-form-wrap">
          <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://blog.libreliu.info"></form>
        </div>
      </nav>
    </div>
  </div>
</header>

    <br>
    <section id="main" class="outer"><article id="paper-reading-paper-reading/icarus-nerf" class="article article-type-paper-reading" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      论文阅读 | ICARUS: NeRF 硬件加速器
      <small class=article-detail-date-index>&nbsp; 2022-10-07</small>
    </h1>
  


        <div class=page-title></div>
        <br>
      </header>
    
    <div class="article-meta">
      <!--<a href="/paper-reading/icarus-nerf/" class="article-date">
  <time datetime="2022-10-06T16:00:00.000Z" itemprop="datePublished">2022-10-07</time>
</a>-->
      <!-- 
--><!-- by blair 160724 -->
      <!-- by blair
      
      -->
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
          <p>论文标题：ICARUS: A Specialized Architecture for Neural Radiance Fields Rendering</p>
          <p>论文来源：SIGGRAPH Asia 2022</p>
          <p>论文作者：Chaolin Rao, Huangjie Yu, Haochuan Wan, Jindong Zhou, Yueyang Zheng, Yu Ma, Anpei Chen, Minye Wu, Binzhe Yuan, Pingqiang Zhou, Xin Lou, Jingyi Yu</p>
          <p><a target="_blank" rel="noopener" href='https://arxiv.org/abs/2203.01414'>论文链接</a></p>
        </blockquote>
      
      
      
        <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇文章介绍了 NeRF 硬件加速的实现。</p>
<h2 id="NeRF-回顾"><a href="#NeRF-回顾" class="headerlink" title="NeRF 回顾"></a>NeRF 回顾</h2><img src="/paper-reading/icarus-nerf/nerf-overview.png" class="" title="NeRF Overview from Original Paper">

<p>Neural Radiance Field，简称 NeRF，最开始在 ECCV 2020 上<a target="_blank" rel="noopener" href="https://www.matthewtancik.com/nerf">被提出</a>，提出了以神经网络编码辐射场的一种技术，并且将其运用到了基于图片的场景重建等多个领域中，是近年来受关注度相当高的一篇工作。</p>
<p>NeRF 的网络部分输入为 5D: 位置 $ (x,y,z) $ 和朝向 $ (\theta, \phi) $，输出为该位置的 RGB 颜色和密度。</p>
<p>NeRF 在给定相机位置下最终渲染的输出用类似体渲染 (Volumetric Rendering) 的办法来实现。</p>
<h3 id="NeRF-体渲染"><a href="#NeRF-体渲染" class="headerlink" title="NeRF 体渲染"></a>NeRF 体渲染</h3><p>对给定的相机光线 $ {\bf r}(t) &#x3D; {\bf o} + t{\bf d} $ 来说，最终输出的颜色 $ {\bf C}(r) $ 以下式表示：</p>
$$
C({\bf r}) = \int_{t_n}^{t_f} T(t) \sigma({\bf r}(t)) {\bf c}({\bf r}(t), {\bf d}) dt 
$$

<p>其中：</p>
<ul>
<li>$ T(t) = \exp (-\int_{t_n}^{t} \sigma({\bf r}(s)) ds ) $ 为光线从 $ t_n $ 能打到 $ t $ 的概率<ul>
<li>比如说，如果射线穿过的部分密度都比较大，那 $ T(t) $ 就会比较小</li>
</ul>
</li>
<li>$ \sigma({\bf r}(t)) $ 是该 $ t $ 对应的点 $ {\bf r}(t) $ 的密度</li>
<li>$ {\bf c}({\bf r}(t), {\bf d}) $ 是网络给定方向和位置后输出的 RGB 颜色值</li>
<li>$ t_n $ 和 $ t_f $ 分别为射线进入和射出 NeRF 有效区域的包围盒时所对应的最近和最远参数值</li>
</ul>
<p>不过这个积分显然不能很容易的解析求解，NeRF 的做法是采用数值积分的那一套。</p>
<p>首先，利用分层抽样 (stratified sampling) 的技术，将 $ [t_n, t_f] $ 分成 $ N $ 个均匀的小区间，然后在每个小区间均匀采样出一个 $ t_i $ 出来。</p>
<p>然后，用下面的量 $ \hat C({\bf r}) $ 来估计上面的 $ C({\bf r}) $：</p>
$$
\hat C({\bf r}) = \sum_{i=1}^{N} T_i (1-\exp(-\sigma_i \delta_i)) {\bf c}_i
$$

<p>其中：</p>
<ul>
<li>$ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_j \delta_j) $</li>
<li>$ \delta_i = t_{i+1} - t_i $ 为两临近采样点的距离</li>
</ul>
<blockquote>
<p>为什么会变成这个形式？可以参考 arXiv 上的 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.02417.pdf">Volume Rendering Digest (for NeRF)</a></p>
</blockquote>
<!-- 没看，TODO: 研究一下 -->

<p>原文中提到，从所有的 $ ({\bf c}_i, \delta_i) $ 对考虑的话，$ \hat C(r) $ 的计算显然是可微的，并且可以看成从最开始一直用 $ \alpha_i &#x3D; 1 - \exp(\sigma_i \delta_i) $ 的透明度往上面做 alpha blending。</p>
<!-- TODO: why alpha value like that -->

<h3 id="NeRF-网络"><a href="#NeRF-网络" class="headerlink" title="NeRF 网络"></a>NeRF 网络</h3><p>网络部分用位置编码 (Positional Encoding) + Coarse MLP + Fine MLP。</p>
<h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p>位置编码用来改善网络对高频细节的学习效果。</p>
<p>位置编码层可以如下描述：</p>
$$
\gamma(p) = (\sin(2^0 \pi p), \cos(2^0 \pi p), ..., \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p))
$$

<h4 id="Coarse-amp-Fine-MLP"><a href="#Coarse-amp-Fine-MLP" class="headerlink" title="Coarse &amp; Fine MLP"></a>Coarse &amp; Fine MLP</h4><p>NeRF 同时使用两个 MLP 来表示场景，一个粗粒度 MLP 和一个细粒度 MLP。</p>
<p>渲染的时候，首先用分层抽样的办法，在粗粒度网络中用前面提到的体渲染方法进行渲染，并且得到输出 $ \hat C_c(r) $：</p>
$$
\hat C_c(r) = \sum_{i=1}^{N_c} w_i c_i, \quad w_i = T_i (1-\exp(\sigma_i \delta_i))
$$

<p>然后，计算归一化权重 $ \hat w_i &#x3D; w_i &#x2F; \sum_{i&#x3D;1}^{N_c} w_i $，并且用计算好的归一化权重作为概率分布函数 (cumulative distribution function)，再在这条直线上采样 $ N_f $ 个位置，将这 $ N_c + N_f $ 个位置送入细粒度 MLP 进行推理，再用前面的办法渲染得到最终的颜色值。</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>采用简单的把 Coarse MLP 和 Fine MLP 与真实值之间的 $ L^2 $ 损失直接加起来的办法。</p>
<h2 id="ICARUS"><a href="#ICARUS" class="headerlink" title="ICARUS"></a>ICARUS</h2><img src="/paper-reading/icarus-nerf/icarus-overview.png" class="" title="ICARUS Overview">

<h3 id="NeRF-计算过程回顾"><a href="#NeRF-计算过程回顾" class="headerlink" title="NeRF 计算过程回顾"></a>NeRF 计算过程回顾</h3><ol>
<li>对像素所发出射线上的采样，得到点 $ ({\bf p}_1, …, {\bf p}_N) $</li>
<li>查询 MLP 网络：$ ({\bf p}_i, {\bf d}_i) \to ({\bf c_i}, \sigma_i) $</li>
<li>进行多次 alpha-blending</li>
</ol>
<h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><p>架构设计时主要有以下目标：</p>
<ol>
<li>“端到端” - 芯片输入位置和方向，输出像素颜色，减少片上片外数据交换的额外开销（计算时间、功耗）</li>
<li>使用定点数 - 有效降低浮点数运算开销</li>
<li>架构设计要一定灵活性，尽量兼容比较多的 NeRF 衍生网络</li>
</ol>
<h4 id="如何使用定点数？"><a href="#如何使用定点数？" class="headerlink" title="如何使用定点数？"></a>如何使用定点数？</h4><p>目前的实现是将在 GPU 上训练好的 NeRF 的权重进行量化 (quantization)，再导出。不过，目前也有一些工作在 quantization-aware training 方面，可能对这个网络的训练过程有所帮助。</p>
<h3 id="位置编码单元-PEU"><a href="#位置编码单元-PEU" class="headerlink" title="位置编码单元 (PEU)"></a>位置编码单元 (PEU)</h3><p>设计位置编码单元 (Positional Encoding Unit, PEU) 的目的是在 PEU 前和 PEU 后的向量维数增加了很多倍（对原 NeRF 来说位置是 20 倍，方向是 8 倍），如果在 ICARUS 内部进行计算的话，可以减少很大一部分外部存储传输，降低传输总用时。</p>
<p>PEU 部件主要在做这件事：</p>
$$
\phi(x; A) = [\cos A^T x, \sin A^T x]
$$

<p>其中 $ A $ 一般为一个行数比列数多的矩阵，用来升维。</p>
<p>PEU 单元对应的设计如下 (Fig. 4(b))：</p>
<img src="/paper-reading/icarus-nerf/peu-overview.png" class="" title="PEU Overview">

<p>可以看到，就是矩阵乘法单元和 CORDIC 单元的组合。</p>
<blockquote>
<p>一些关于矩阵乘和 CORDIC 单元的大概印象：<br>矩阵乘：有很多工作，比如搜索 Systolic array 等等</p>
<ul>
<li>不过我不清楚 SOTA 情况<br>CORDIC: <a target="_blank" rel="noopener" href="https://zipcpu.com/dsp/2017/08/30/cordic.html">https://zipcpu.com/dsp/2017/08/30/cordic.html</a></li>
<li>不过我也不清楚 SOTA 情况</li>
</ul>
</blockquote>
<p>具体设计上来说，ICARUS 支持对 dim&#x3D;3 和 dim&#x3D;6 的两种输入进行位置编码，并且扩展到 dim&#x3D;128。PEU 内部设计有两个 3x128 的内存块和 6 组 MAC (Multiply-ACcumulate) 单元，当计算 dim&#x3D;6 的输入时会全部启用，当计算 dim&#x3D;3 的输入时只启用一半。</p>
<h3 id="MLP-Engine"><a href="#MLP-Engine" class="headerlink" title="MLP Engine"></a>MLP Engine</h3><p>MLP 引擎主要进行 $ f(Wx+b) $ 类型的计算。</p>
<p>MLP 引擎包含有：</p>
<ul>
<li>一个 Multi-output Network block (MONB)，负责计算中间的隐藏层</li>
<li>一个 Single-output network block (SONB)，负责计算最后的输出层<ul>
<li>不继续用 MONB 的原因是，全连接的 MONB 比只输出一个数字的 SONB 面积要大得多</li>
</ul>
</li>
<li>两个 activation memory block</li>
</ul>
<p>对于 MLP 计算来说，实现是这样的：</p>
<img src="/paper-reading/icarus-nerf/mlp-compution-overview.png" class="" title="MLP Overview">

<p>首先，将 MLP 的权重拆成 64 x 64 的小块，方便硬件上的复用，并且同样的权重可以被多组输入向量复用，从而降低内存带宽开销，代价方面只需要暂存该 batch 内的中间结果就可以（这里选择 <code>batch_size=128</code>）。</p>
<p>每个 64 x 64 的矩阵-向量乘法再进行分片，变成按矩阵列分割的 64 个列向量 - 向量的内积乘法（即 $ [\alpha_1 … \alpha_{64}] [x_1 … x_{64}]^T $，每个 $ \alpha_i x_i $ 的部分和用一个 RMCM 模块实现：</p>
<img src="/paper-reading/icarus-nerf/working_principle_rmcm.png" class="" title="RMCM Principle">

<p>大概来说，是因为乘法可以变成移位加法：</p>
$$
3x = 1x << 1 + 1x
$$

<p>所以权重 load 进来的作用就是预先选择好路径上的移位和加法器，然后数据从这些器件中流过去就行。</p>
<p>另一个优化是高一半的移位和加法路径直接用上一次的值来替换，然后网络训练的时候也作此改动。这样可以节省 1&#x2F;3 的面积，同时输出基本没有视觉质量损失。</p>
<p>SONB 的架构基本上和 MONB 差不多，只是 RMCM 块用不到了，用普通的向量乘法块就可以了。</p>
<img src="/paper-reading/icarus-nerf/sonb-overview.png" class="" title="SONB Overview">

<h3 id="Volume-Rendering-Unit"><a href="#Volume-Rendering-Unit" class="headerlink" title="Volume Rendering Unit"></a>Volume Rendering Unit</h3><img src="/paper-reading/icarus-nerf/VRU-Overview.png" class="" title="VRU Overview">

<p>VRU 模块主要要负责下面的计算：</p>
<img src="/paper-reading/icarus-nerf/VRU-classic.png" class="" title="VRU Classic">

<p>这里，他处理成下面的形式：</p>
<img src="/paper-reading/icarus-nerf/VRU-rewrite.png" class="" title="VRU Rewrite">

<p>然后用上面的网络计算。</p>
<h3 id="原型验证"><a href="#原型验证" class="headerlink" title="原型验证"></a>原型验证</h3><img src="/paper-reading/icarus-nerf/FPGA-prototyping.png" class="">

<p>验证平台使用的是 Synopsys HAPS-80 S104，验证时使用的工艺是 40nm CMOS 工艺。</p>
<img src="/paper-reading/icarus-nerf/icarus-comparation.png" class="">

<!-- 
#### 调整到 Surface Light Field 任务

> TODO

-->
      
     <!-- by blair add this if sentence at 20160725 -->
      <br>
      
<!-- <div id="bottom-donation-section"> -->
<!-- <span style="font-size: 1.0em; padding:0em 1em 0.5em 1em; margin: 0 auto;">
  <strong style="vertical-align: top;">分享到:</strong>
    <div class="j_handlclick"  style="background: url(/images/logos/share_facebook_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.facebook.com/sharer/sharer.php?u=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_twitter_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://twitter.com/intent/tweet?url=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_line_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://www.addtoany.com/add_to/line?linkurl=" target="_blank">
    </div>
    <div class="j_handlclick"  style="background: url(/images/logos/share_wechat_icon.jpg);background-size: contain;display: inline-block; width:50px; height:50px" href="https://api.addthis.com/oexchange/0.8/forward/wechat/offer?url=" target="_blank">
    </div>
  <br>  
  <br>  
  &nbsp;&nbsp;如果您觉得这篇文章对您的学习很有帮助, 请您也分享它, 让它能再次帮助到更多的需要学习的人.
您的<a href="/support/"><strong>支持</strong></a>将鼓励我继续创作 !
  <br>  

</span> -->
<!--
<h3 id="bottom-donation-title">支持 让文章变得更优质</h3>
<div>
<a id="bottom-donation-button" href="/support">点我 赞助 作者</a>
</div>
-->
<!-- </div> -->
<!-- <div class="well"> -->
  <!--
  原创文章，转载请注明： 转载自<a target="_blank" rel="noopener" href="http://52binge.github.io"> Blair Chan's Blog</a>，作者：
  <a href="/about">Blair Chan</a> <br>
  -->
  <!-- 本文基于<a target="_blank" title="Creative Commons Attribution 4.0 international License" href="https://creativecommons.org/licenses/by-nc/4.0/">署名4.0国际许可协议</a>发布，转载请保留本文署名和文章链接。 如您有任何授权方面的协商，请邮件联系我。 -->
<!-- </div> -->
 <!-- by blair add 160724-->
    
    </div>
    
      <div class="article-toc">
        <h3>Contents</h3>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NeRF-%E5%9B%9E%E9%A1%BE"><span class="toc-text">NeRF 回顾</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#NeRF-%E4%BD%93%E6%B8%B2%E6%9F%93"><span class="toc-text">NeRF 体渲染</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NeRF-%E7%BD%91%E7%BB%9C"><span class="toc-text">NeRF 网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Coarse-amp-Fine-MLP"><span class="toc-text">Coarse &amp; Fine MLP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ICARUS"><span class="toc-text">ICARUS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#NeRF-%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E5%9B%9E%E9%A1%BE"><span class="toc-text">NeRF 计算过程回顾</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-text">架构设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%9A%E7%82%B9%E6%95%B0%EF%BC%9F"><span class="toc-text">如何使用定点数？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%8D%95%E5%85%83-PEU"><span class="toc-text">位置编码单元 (PEU)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MLP-Engine"><span class="toc-text">MLP Engine</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Volume-Rendering-Unit"><span class="toc-text">Volume Rendering Unit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%9E%8B%E9%AA%8C%E8%AF%81"><span class="toc-text">原型验证</span></a></li></ol></li></ol>
      </div>
    
    
      <footer class="article-footer">
        <!-- <div class="well" style="width:100px; height:30px;"></div>  by blair-->
        
 <!-- by blair add 160724-->
        <!--
        <div style="width:100px; height:30px;"></div> by blair add 160724
        -->
        

      </footer>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/paper-reading/pbr-feature-line/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">论文阅读 | 基于物理的特征线渲染&nbsp;<span>&gt;</span></div>
    </a>
  
</nav>

  
</article>

</section>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 Libre Liu&nbsp;
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, theme by <a target="_blank" rel="noopener" href="http://github.com/52binge/hexo-theme-blairos">blairos</a>
    </div>
  </div>
</footer>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX"],
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true
    }
  });
</script>
<script>
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ],
      displayMath: [ ['$$','$$']],
      processEscapes: true,
    },
    options: {
    renderActions: {
      findScript: [10, function (doc) {
        for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
          const display = !!node.type.match(/; *mode=display/);
          const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          const text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
  };
</script>
<script type="text/javascript" id="MathJax-script" src="/js/mathjax/tex-chtml.js">
</script>

    

<script src="/js/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
</body>
</html>
